{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a2559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# Import models\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Import XGBoost and CatBoost\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Import LightGBM\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Import metrics for evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Import numpy and pandas (for preprocessing and evaluation)\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c27024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/train.csv\")\n",
    "test_df=pd.read_csv(\"data/test.csv\")\n",
    "submission_df=pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4141d84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 750000 entries, 0 to 749999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   id                           750000 non-null  int64  \n",
      " 1   Podcast_Name                 750000 non-null  object \n",
      " 2   Episode_Title                750000 non-null  object \n",
      " 3   Episode_Length_minutes       662907 non-null  float64\n",
      " 4   Genre                        750000 non-null  object \n",
      " 5   Host_Popularity_percentage   750000 non-null  float64\n",
      " 6   Publication_Day              750000 non-null  object \n",
      " 7   Publication_Time             750000 non-null  object \n",
      " 8   Guest_Popularity_percentage  603970 non-null  float64\n",
      " 9   Number_of_Ads                749999 non-null  float64\n",
      " 10  Episode_Sentiment            750000 non-null  object \n",
      " 11  Listening_Time_minutes       750000 non-null  float64\n",
      "dtypes: float64(5), int64(1), object(6)\n",
      "memory usage: 68.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506eeb5d",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7479ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode_Length_minutes 0.1161  % missing values\n",
      "Guest_Popularity_percentage 0.1947  % missing values\n",
      "Number_of_Ads 0.0  % missing values\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Here we will check the percentage of nan values present in each feature\n",
    "## 1 -step make the list of features which has missing values\n",
    "features_with_na=[features for features in df.columns if df[features].isnull().sum()>0]\n",
    "## 2- step print the feature name and the percentage of missing values\n",
    "\n",
    "for feature in features_with_na:\n",
    "    print(feature, np.round(df[feature].isnull().mean(), 4),  ' % missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e864fab8",
   "metadata": {},
   "source": [
    "there are some missing value but since i don't want to remove and loose any information i will fill them with median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d31fa645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with median for specified columns in both dataframes\n",
    "columns_to_fill = ['Number_of_Ads', 'Episode_Length_minutes', 'Guest_Popularity_percentage']\n",
    "# Calculate medians from TRAINING DATA only\n",
    "train_medians = df[columns_to_fill].median()\n",
    "\n",
    "# Apply training medians to BOTH datasets\n",
    "df[columns_to_fill] = df[columns_to_fill].fillna(train_medians)\n",
    "test_df[columns_to_fill] = test_df[columns_to_fill].fillna(train_medians)  # Use train's median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05d9b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 750000 entries, 0 to 749999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   id                           750000 non-null  int64  \n",
      " 1   Podcast_Name                 750000 non-null  object \n",
      " 2   Episode_Title                750000 non-null  object \n",
      " 3   Episode_Length_minutes       750000 non-null  float64\n",
      " 4   Genre                        750000 non-null  object \n",
      " 5   Host_Popularity_percentage   750000 non-null  float64\n",
      " 6   Publication_Day              750000 non-null  object \n",
      " 7   Publication_Time             750000 non-null  object \n",
      " 8   Guest_Popularity_percentage  750000 non-null  float64\n",
      " 9   Number_of_Ads                750000 non-null  float64\n",
      " 10  Episode_Sentiment            750000 non-null  object \n",
      " 11  Listening_Time_minutes       750000 non-null  float64\n",
      "dtypes: float64(5), int64(1), object(6)\n",
      "memory usage: 68.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a079c8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250000 entries, 0 to 249999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   id                           250000 non-null  int64  \n",
      " 1   Podcast_Name                 250000 non-null  object \n",
      " 2   Episode_Title                250000 non-null  object \n",
      " 3   Episode_Length_minutes       250000 non-null  float64\n",
      " 4   Genre                        250000 non-null  object \n",
      " 5   Host_Popularity_percentage   250000 non-null  float64\n",
      " 6   Publication_Day              250000 non-null  object \n",
      " 7   Publication_Time             250000 non-null  object \n",
      " 8   Guest_Popularity_percentage  250000 non-null  float64\n",
      " 9   Number_of_Ads                250000 non-null  float64\n",
      " 10  Episode_Sentiment            250000 non-null  object \n",
      "dtypes: float64(4), int64(1), object(6)\n",
      "memory usage: 21.0+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27477d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Podcast_Name</th>\n",
       "      <th>Episode_Title</th>\n",
       "      <th>Episode_Length_minutes</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Host_Popularity_percentage</th>\n",
       "      <th>Publication_Day</th>\n",
       "      <th>Publication_Time</th>\n",
       "      <th>Guest_Popularity_percentage</th>\n",
       "      <th>Number_of_Ads</th>\n",
       "      <th>Episode_Sentiment</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mystery Matters</td>\n",
       "      <td>Episode 98</td>\n",
       "      <td>63.84</td>\n",
       "      <td>True Crime</td>\n",
       "      <td>74.81</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Night</td>\n",
       "      <td>53.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>31.41998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Joke Junction</td>\n",
       "      <td>Episode 26</td>\n",
       "      <td>119.80</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>66.95</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>75.95</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>88.01241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Study Sessions</td>\n",
       "      <td>Episode 16</td>\n",
       "      <td>73.90</td>\n",
       "      <td>Education</td>\n",
       "      <td>69.97</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>8.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>44.92531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Digital Digest</td>\n",
       "      <td>Episode 45</td>\n",
       "      <td>67.17</td>\n",
       "      <td>Technology</td>\n",
       "      <td>57.22</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>78.70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>46.27824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mind &amp; Body</td>\n",
       "      <td>Episode 86</td>\n",
       "      <td>110.51</td>\n",
       "      <td>Health</td>\n",
       "      <td>80.07</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>58.68</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>75.61031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     Podcast_Name Episode_Title  Episode_Length_minutes       Genre  \\\n",
       "0   0  Mystery Matters    Episode 98                   63.84  True Crime   \n",
       "1   1    Joke Junction    Episode 26                  119.80      Comedy   \n",
       "2   2   Study Sessions    Episode 16                   73.90   Education   \n",
       "3   3   Digital Digest    Episode 45                   67.17  Technology   \n",
       "4   4      Mind & Body    Episode 86                  110.51      Health   \n",
       "\n",
       "   Host_Popularity_percentage Publication_Day Publication_Time  \\\n",
       "0                       74.81        Thursday            Night   \n",
       "1                       66.95        Saturday        Afternoon   \n",
       "2                       69.97         Tuesday          Evening   \n",
       "3                       57.22          Monday          Morning   \n",
       "4                       80.07          Monday        Afternoon   \n",
       "\n",
       "   Guest_Popularity_percentage  Number_of_Ads Episode_Sentiment  \\\n",
       "0                        53.58            0.0          Positive   \n",
       "1                        75.95            2.0          Negative   \n",
       "2                         8.97            0.0          Negative   \n",
       "3                        78.70            2.0          Positive   \n",
       "4                        58.68            3.0           Neutral   \n",
       "\n",
       "   Listening_Time_minutes  \n",
       "0                31.41998  \n",
       "1                88.01241  \n",
       "2                44.92531  \n",
       "3                46.27824  \n",
       "4                75.61031  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fd9971b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                             750000\n",
       "Podcast_Name                       48\n",
       "Episode_Title                     100\n",
       "Episode_Length_minutes          12268\n",
       "Genre                              10\n",
       "Host_Popularity_percentage       8038\n",
       "Publication_Day                     7\n",
       "Publication_Time                    4\n",
       "Guest_Popularity_percentage     10019\n",
       "Number_of_Ads                      12\n",
       "Episode_Sentiment                   3\n",
       "Listening_Time_minutes          42807\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af418c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c0f2e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Episode_Length_minutes</th>\n",
       "      <th>Host_Popularity_percentage</th>\n",
       "      <th>Guest_Popularity_percentage</th>\n",
       "      <th>Number_of_Ads</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>374999.500000</td>\n",
       "      <td>64.427546</td>\n",
       "      <td>59.859901</td>\n",
       "      <td>52.498047</td>\n",
       "      <td>1.348854</td>\n",
       "      <td>45.437406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>216506.495284</td>\n",
       "      <td>30.996996</td>\n",
       "      <td>22.873098</td>\n",
       "      <td>25.537152</td>\n",
       "      <td>1.151130</td>\n",
       "      <td>27.138306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>187499.750000</td>\n",
       "      <td>39.420000</td>\n",
       "      <td>39.410000</td>\n",
       "      <td>34.550000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.178350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>374999.500000</td>\n",
       "      <td>63.840000</td>\n",
       "      <td>60.050000</td>\n",
       "      <td>53.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>43.379460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>562499.250000</td>\n",
       "      <td>90.310000</td>\n",
       "      <td>79.530000</td>\n",
       "      <td>71.040000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>64.811580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>749999.000000</td>\n",
       "      <td>325.240000</td>\n",
       "      <td>119.460000</td>\n",
       "      <td>119.910000</td>\n",
       "      <td>103.910000</td>\n",
       "      <td>119.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  Episode_Length_minutes  Host_Popularity_percentage  \\\n",
       "count  750000.000000           750000.000000               750000.000000   \n",
       "mean   374999.500000               64.427546                   59.859901   \n",
       "std    216506.495284               30.996996                   22.873098   \n",
       "min         0.000000                0.000000                    1.300000   \n",
       "25%    187499.750000               39.420000                   39.410000   \n",
       "50%    374999.500000               63.840000                   60.050000   \n",
       "75%    562499.250000               90.310000                   79.530000   \n",
       "max    749999.000000              325.240000                  119.460000   \n",
       "\n",
       "       Guest_Popularity_percentage  Number_of_Ads  Listening_Time_minutes  \n",
       "count                750000.000000  750000.000000           750000.000000  \n",
       "mean                     52.498047       1.348854               45.437406  \n",
       "std                      25.537152       1.151130               27.138306  \n",
       "min                       0.000000       0.000000                0.000000  \n",
       "25%                      34.550000       0.000000               23.178350  \n",
       "50%                      53.580000       1.000000               43.379460  \n",
       "75%                      71.040000       2.000000               64.811580  \n",
       "max                     119.910000     103.910000              119.970000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36b740d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 6 numerical features : ['id', 'Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Listening_Time_minutes']\n",
      "\n",
      "We have 6 categorical features : ['Podcast_Name', 'Episode_Title', 'Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\n"
     ]
    }
   ],
   "source": [
    "numeric_features = [feature for feature in df.columns if df[feature].dtype != 'O']\n",
    "categorical_features = [feature for feature in df.columns if df[feature].dtype == 'O']\n",
    "\n",
    "# print columns\n",
    "print('We have {} numerical features : {}'.format(len(numeric_features), numeric_features))\n",
    "print('\\nWe have {} categorical features : {}'.format(len(categorical_features), categorical_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb051ba",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb16534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_to_num = {\n",
    "    'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, \n",
    "    'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6\n",
    "}\n",
    "\n",
    "df['Publication_Day'] = df['Publication_Day'].str.capitalize()\n",
    "df['Day_num'] = df['Publication_Day'].map(day_to_num)\n",
    "if df['Day_num'].isna().any():\n",
    "    print(\"Warning: Missing or invalid Publication_Day values\")\n",
    "    df['Day_num'].fillna(0, inplace=True)  # Default to Monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db3663ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Podcast_Name</th>\n",
       "      <th>Episode_Title</th>\n",
       "      <th>Episode_Length_minutes</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Host_Popularity_percentage</th>\n",
       "      <th>Publication_Day</th>\n",
       "      <th>Publication_Time</th>\n",
       "      <th>Guest_Popularity_percentage</th>\n",
       "      <th>Number_of_Ads</th>\n",
       "      <th>Episode_Sentiment</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "      <th>Day_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mystery Matters</td>\n",
       "      <td>Episode 98</td>\n",
       "      <td>63.84</td>\n",
       "      <td>True Crime</td>\n",
       "      <td>74.81</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Night</td>\n",
       "      <td>53.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>31.41998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Joke Junction</td>\n",
       "      <td>Episode 26</td>\n",
       "      <td>119.80</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>66.95</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>75.95</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>88.01241</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Study Sessions</td>\n",
       "      <td>Episode 16</td>\n",
       "      <td>73.90</td>\n",
       "      <td>Education</td>\n",
       "      <td>69.97</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>8.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>44.92531</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Digital Digest</td>\n",
       "      <td>Episode 45</td>\n",
       "      <td>67.17</td>\n",
       "      <td>Technology</td>\n",
       "      <td>57.22</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>78.70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>46.27824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mind &amp; Body</td>\n",
       "      <td>Episode 86</td>\n",
       "      <td>110.51</td>\n",
       "      <td>Health</td>\n",
       "      <td>80.07</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>58.68</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>75.61031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     Podcast_Name Episode_Title  Episode_Length_minutes       Genre  \\\n",
       "0   0  Mystery Matters    Episode 98                   63.84  True Crime   \n",
       "1   1    Joke Junction    Episode 26                  119.80      Comedy   \n",
       "2   2   Study Sessions    Episode 16                   73.90   Education   \n",
       "3   3   Digital Digest    Episode 45                   67.17  Technology   \n",
       "4   4      Mind & Body    Episode 86                  110.51      Health   \n",
       "\n",
       "   Host_Popularity_percentage Publication_Day Publication_Time  \\\n",
       "0                       74.81        Thursday            Night   \n",
       "1                       66.95        Saturday        Afternoon   \n",
       "2                       69.97         Tuesday          Evening   \n",
       "3                       57.22          Monday          Morning   \n",
       "4                       80.07          Monday        Afternoon   \n",
       "\n",
       "   Guest_Popularity_percentage  Number_of_Ads Episode_Sentiment  \\\n",
       "0                        53.58            0.0          Positive   \n",
       "1                        75.95            2.0          Negative   \n",
       "2                         8.97            0.0          Negative   \n",
       "3                        78.70            2.0          Positive   \n",
       "4                        58.68            3.0           Neutral   \n",
       "\n",
       "   Listening_Time_minutes  Day_num  \n",
       "0                31.41998        3  \n",
       "1                88.01241        5  \n",
       "2                44.92531        1  \n",
       "3                46.27824        0  \n",
       "4                75.61031        0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4fb8e",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03341183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from textblob import TextBlob\n",
    "import optuna\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Global flag for target transformation\n",
    "TARGET_TRANSFORMED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e43678a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Training set shape: (600000, 10), Validation set shape: (150000, 10)\n",
      "Original y_train skewness: 0.3502503762244965\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    try:\n",
    "        train_df = pd.read_csv('data/train.csv')\n",
    "        test_df = pd.read_csv('data/test.csv')\n",
    "        print(\"Data loaded successfully.\")\n",
    "        return train_df, test_df\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: train.csv or test.csv not found. Make sure they are in a 'data' subdirectory.\")\n",
    "        exit()\n",
    "\n",
    "# Load data\n",
    "train_df, test_df = load_data()\n",
    "\n",
    "# Keep test IDs\n",
    "test_ids = test_df['id']\n",
    "\n",
    "# Prepare data\n",
    "X = train_df.drop(columns=['Listening_Time_minutes', 'id'])\n",
    "y = train_df['Listening_Time_minutes']\n",
    "X_submission_test_features = test_df.drop(columns=['id'])\n",
    "\n",
    "# Split data\n",
    "X_train_df, X_val_df, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Training set shape: {X_train_df.shape}, Validation set shape: {X_val_df.shape}\")\n",
    "\n",
    "# Check target skewness\n",
    "print(\"Original y_train skewness:\", y_train.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6118ce7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features for training data...\n"
     ]
    }
   ],
   "source": [
    "def engineer_features(df, is_train=True, target_series=None, vectorizer=None, mean_listen_time=None, mean_genre=None, scaler=None):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Cyclical encoding for Publication_Day\n",
    "    day_to_num = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6}\n",
    "    df['Publication_Day'] = df['Publication_Day'].str.capitalize().fillna('Unknown')\n",
    "    df['Day_num'] = df['Publication_Day'].map(day_to_num).fillna(0)\n",
    "    df['Day_sin'] = np.sin(2 * np.pi * df['Day_num'] / 7)\n",
    "    df['Day_cos'] = np.cos(2 * np.pi * df['Day_num'] / 7)\n",
    "\n",
    "    # Cyclical encoding for Publication_Time\n",
    "    time_to_hour = {'Night': 20, 'Morning': 8, 'Afternoon': 14, 'Evening': 18}\n",
    "    df['Publication_Time'] = df['Publication_Time'].fillna('Unknown')\n",
    "    df['Hour'] = df['Publication_Time'].map(time_to_hour).fillna(12)\n",
    "    df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
    "    df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
    "\n",
    "    # Title features\n",
    "    df['Episode_Title'] = df['Episode_Title'].fillna('')\n",
    "    df['Title_Length'] = df['Episode_Title'].apply(len)\n",
    "    df['Title_Word_Count'] = df['Episode_Title'].apply(lambda x: len(x.split()))\n",
    "    df['Title_Sentiment'] = df['Episode_Title'].apply(lambda x: TextBlob(x).sentiment.polarity if x else 0)\n",
    "\n",
    "    # New features\n",
    "    df['Podcast_Frequency'] = df['Podcast_Name'].map(df['Podcast_Name'].value_counts())\n",
    "    df['Genre_Length'] = df['Genre'].astype(str) + '_' + pd.cut(df['Episode_Length_minutes'], bins=3, labels=['short', 'medium', 'long']).astype(str)\n",
    "    df['Genre_Length'] = df['Genre_Length'].astype('category').cat.codes\n",
    "\n",
    "    # Target encoding\n",
    "    if is_train:\n",
    "        if target_series is None:\n",
    "            raise ValueError(\"target_series must be provided when is_train=True.\")\n",
    "        temp_target = target_series.copy()\n",
    "        global_mean = temp_target.mean()\n",
    "        mean_listen_time = temp_target.groupby(df['Podcast_Name'].fillna('Unknown')).mean().to_dict()\n",
    "        mean_genre = temp_target.groupby(df['Genre'].fillna('Unknown')).mean().to_dict()\n",
    "        mean_listen_time['global'] = global_mean\n",
    "        mean_genre['global'] = global_mean\n",
    "    else:\n",
    "        if mean_listen_time is None or mean_genre is None:\n",
    "            raise ValueError(\"mean_listen_time and mean_genre must be provided when is_train=False.\")\n",
    "        global_mean = mean_listen_time.get('global', 0)\n",
    "\n",
    "    df['Podcast_Mean_Listen'] = df['Podcast_Name'].fillna('Unknown').map(mean_listen_time).fillna(global_mean)\n",
    "    df['Genre_Mean_Listen'] = df['Genre'].fillna('Unknown').map(mean_genre).fillna(global_mean)\n",
    "\n",
    "    # Interaction features\n",
    "    num_cols_for_interactions = ['Host_Popularity_percentage', 'Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads']\n",
    "    for col in num_cols_for_interactions:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "    df['Host_Pop_x_Length'] = df['Host_Popularity_percentage'] * df['Episode_Length_minutes']\n",
    "    df['Guest_Pop_x_Length'] = df['Guest_Popularity_percentage'] * df['Episode_Length_minutes']\n",
    "    df['Ads_x_Length'] = df['Number_of_Ads'] * df['Episode_Length_minutes']\n",
    "    df['Host_Guest_Pop'] = df['Host_Popularity_percentage'] * df['Guest_Popularity_percentage']\n",
    "\n",
    "    # Normalize interactions\n",
    "    interaction_cols = ['Host_Pop_x_Length', 'Guest_Pop_x_Length', 'Ads_x_Length', 'Host_Guest_Pop']\n",
    "    df[interaction_cols] = df[interaction_cols].fillna(0)\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        df[interaction_cols] = scaler.fit_transform(df[interaction_cols])\n",
    "    else:\n",
    "        if scaler is None:\n",
    "            raise ValueError(\"scaler must be provided when is_train=False.\")\n",
    "        df[interaction_cols] = scaler.transform(df[interaction_cols])\n",
    "\n",
    "    # Episode sentiment\n",
    "    df['Episode_Sentiment_Score'] = df['Episode_Sentiment'].map({'Negative': -1, 'Neutral': 0, 'Positive': 1}).fillna(0)\n",
    "\n",
    "    # TF-IDF vectorization\n",
    "    if is_train:\n",
    "        vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1,3), min_df=5, stop_words='english')\n",
    "        X_text = vectorizer.fit_transform(df['Episode_Title'])\n",
    "    else:\n",
    "        if vectorizer is None:\n",
    "            raise ValueError(\"vectorizer must be provided when is_train=False.\")\n",
    "        X_text = vectorizer.transform(df['Episode_Title'])\n",
    "\n",
    "    # Combine features\n",
    "    numerical_cols = [\n",
    "        'Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage',\n",
    "        'Number_of_Ads', 'Day_sin', 'Day_cos', 'Hour_sin', 'Hour_cos', 'Title_Length',\n",
    "        'Title_Word_Count', 'Title_Sentiment', 'Podcast_Mean_Listen', 'Genre_Mean_Listen',\n",
    "        'Host_Pop_x_Length', 'Guest_Pop_x_Length', 'Ads_x_Length', 'Host_Guest_Pop',\n",
    "        'Episode_Sentiment_Score', 'Podcast_Frequency', 'Genre_Length'\n",
    "    ]\n",
    "    existing_numerical_cols = [col for col in numerical_cols if col in df.columns]\n",
    "    X_numerical = df[existing_numerical_cols].fillna(0).values\n",
    "    X = hstack([X_text, X_numerical]).tocsr()\n",
    "\n",
    "    if is_train:\n",
    "        return X, vectorizer, mean_listen_time, mean_genre, scaler\n",
    "    return X, None, None, None, None\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"Engineering features for training data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7abd43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"data/train.csv\")\n",
    "test=pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29eb46cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Engineering features for training data...\n",
      "Engineering features for test data...\n",
      "Starting cross-validation...\n",
      "Fold 1\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.447808\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.447808\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.438867\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.438309\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.467486\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.452173\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.442205\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Fold 1 RMSE: 12.981267148534828\n",
      "Fold 2\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.421359\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.421359\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.411725\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.395852\n",
      "[LightGBM] [Info] Total Bins 2417\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.439112\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.435957\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.424149\n",
      "Fold 2 RMSE: 13.025007442812266\n",
      "Fold 3\n",
      "[LightGBM] [Info] Total Bins 2417\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.439263\n",
      "[LightGBM] [Info] Total Bins 2417\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.439263\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.422351\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.430497\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.449522\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.445839\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.448103\n",
      "Fold 3 RMSE: 13.020940582257222\n",
      "Fold 4\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.453215\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.453215\n",
      "[LightGBM] [Info] Total Bins 2415\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.453825\n",
      "[LightGBM] [Info] Total Bins 2415\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.435483\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.485342\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.451680\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.439742\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Fold 4 RMSE: 13.02741149797767\n",
      "Fold 5\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.425388\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.425388\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.434119\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.414586\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.431788\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.411392\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.435051\n",
      "Fold 5 RMSE: 12.98298529887782\n",
      "Training final LightGBM model...\n",
      "[LightGBM] [Info] Total Bins 2416\n",
      "[LightGBM] [Info] Number of data points in the train set: 750000, number of used features: 201\n",
      "[LightGBM] [Info] Start training from score 45.437406\n",
      "Mean CV RMSE: 13.00752239409196\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from textblob import TextBlob\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Feature engineering function\n",
    "def engineer_features(df, is_train=True, target_series=None, vectorizer=None, mean_listen_time=None, mean_genre=None, scaler=None):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Cyclical encoding for Publication_Day\n",
    "    day_to_num = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6}\n",
    "    df['Publication_Day'] = df['Publication_Day'].str.capitalize().fillna('Unknown')\n",
    "    df['Day_num'] = df['Publication_Day'].map(day_to_num).fillna(0)\n",
    "    df['Day_sin'] = np.sin(2 * np.pi * df['Day_num'] / 7)\n",
    "    df['Day_cos'] = np.cos(2 * np.pi * df['Day_num'] / 7)\n",
    "\n",
    "    # Cyclical encoding for Publication_Time\n",
    "    time_to_hour = {'Night': 20, 'Morning': 8, 'Afternoon': 14, 'Evening': 18}\n",
    "    df['Publication_Time'] = df['Publication_Time'].fillna('Unknown')\n",
    "    df['Hour'] = df['Publication_Time'].map(time_to_hour).fillna(12)\n",
    "    df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
    "    df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
    "\n",
    "    # Title features\n",
    "    df['Episode_Title'] = df['Episode_Title'].fillna('')\n",
    "    df['Title_Length'] = df['Episode_Title'].apply(len)\n",
    "    df['Title_Word_Count'] = df['Episode_Title'].apply(lambda x: len(x.split()))\n",
    "    df['Title_Sentiment'] = df['Episode_Title'].apply(lambda x: TextBlob(x).sentiment.polarity if x else 0)\n",
    "\n",
    "    # New features\n",
    "    df['Podcast_Frequency'] = df['Podcast_Name'].map(df['Podcast_Name'].value_counts())\n",
    "    df['Genre_Length'] = df['Genre'].astype(str) + '_' + pd.cut(df['Episode_Length_minutes'], bins=3, labels=['short', 'medium', 'long']).astype(str)\n",
    "    df['Genre_Length'] = df['Genre_Length'].astype('category').cat.codes\n",
    "\n",
    "    # Target encoding\n",
    "    if is_train:\n",
    "        if target_series is None:\n",
    "            raise ValueError(\"target_series must be provided when is_train=True.\")\n",
    "        temp_target = target_series.copy()\n",
    "        global_mean = temp_target.mean()\n",
    "        mean_listen_time = temp_target.groupby(df['Podcast_Name'].fillna('Unknown')).mean().to_dict()\n",
    "        mean_genre = temp_target.groupby(df['Genre'].fillna('Unknown')).mean().to_dict()\n",
    "        mean_listen_time['global'] = global_mean\n",
    "        mean_genre['global'] = global_mean\n",
    "    else:\n",
    "        if mean_listen_time is None or mean_genre is None:\n",
    "            raise ValueError(\"mean_listen_time and mean_genre must be provided when is_train=False.\")\n",
    "        global_mean = mean_listen_time.get('global', 0)\n",
    "\n",
    "    df['Podcast_Mean_Listen'] = df['Podcast_Name'].fillna('Unknown').map(mean_listen_time).fillna(global_mean)\n",
    "    df['Genre_Mean_Listen'] = df['Genre'].fillna('Unknown').map(mean_genre).fillna(global_mean)\n",
    "\n",
    "    # Interaction features\n",
    "    num_cols_for_interactions = ['Host_Popularity_percentage', 'Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads']\n",
    "    for col in num_cols_for_interactions:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "    df['Host_Pop_x_Length'] = df['Host_Popularity_percentage'] * df['Episode_Length_minutes']\n",
    "    df['Guest_Pop_x_Length'] = df['Guest_Popularity_percentage'] * df['Episode_Length_minutes']\n",
    "    df['Ads_x_Length'] = df['Number_of_Ads'] * df['Episode_Length_minutes']\n",
    "    df['Host_Guest_Pop'] = df['Host_Popularity_percentage'] * df['Guest_Popularity_percentage']\n",
    "\n",
    "    # Normalize interactions\n",
    "    interaction_cols = ['Host_Pop_x_Length', 'Guest_Pop_x_Length', 'Ads_x_Length', 'Host_Guest_Pop']\n",
    "    df[interaction_cols] = df[interaction_cols].fillna(0)\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        df[interaction_cols] = scaler.fit_transform(df[interaction_cols])\n",
    "    else:\n",
    "        if scaler is None:\n",
    "            raise ValueError(\"scaler must be provided when is_train=False.\")\n",
    "        df[interaction_cols] = scaler.transform(df[interaction_cols])\n",
    "\n",
    "    # Episode sentiment\n",
    "    df['Episode_Sentiment_Score'] = df['Episode_Sentiment'].map({'Negative': -1, 'Neutral': 0, 'Positive': 1}).fillna(0)\n",
    "\n",
    "    # TF-IDF vectorization\n",
    "    if is_train:\n",
    "        vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1,3), min_df=5, stop_words='english')\n",
    "        X_text = vectorizer.fit_transform(df['Episode_Title'])\n",
    "    else:\n",
    "        if vectorizer is None:\n",
    "            raise ValueError(\"vectorizer must be provided when is_train=False.\")\n",
    "        X_text = vectorizer.transform(df['Episode_Title'])\n",
    "\n",
    "    # Combine features\n",
    "    numerical_cols = [\n",
    "        'Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage',\n",
    "        'Number_of_Ads', 'Day_sin', 'Day_cos', 'Hour_sin', 'Hour_cos', 'Title_Length',\n",
    "        'Title_Word_Count', 'Title_Sentiment', 'Podcast_Mean_Listen', 'Genre_Mean_Listen',\n",
    "        'Host_Pop_x_Length', 'Guest_Pop_x_Length', 'Ads_x_Length', 'Host_Guest_Pop',\n",
    "        'Episode_Sentiment_Score', 'Podcast_Frequency', 'Genre_Length'\n",
    "    ]\n",
    "    existing_numerical_cols = [col for col in numerical_cols if col in df.columns]\n",
    "    X_numerical = df[existing_numerical_cols].fillna(0).values\n",
    "    X = hstack([X_text, X_numerical]).tocsr()\n",
    "\n",
    "    if is_train:\n",
    "        return X, vectorizer, mean_listen_time, mean_genre, scaler\n",
    "    return X, None, None, None, None\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"Engineering features for training data...\")\n",
    "X, vectorizer, mean_listen_time, mean_genre, scaler = engineer_features(\n",
    "    df=train,\n",
    "    is_train=True,\n",
    "    target_series=train['Listening_Time_minutes']\n",
    ")\n",
    "\n",
    "print(\"Engineering features for test data...\")\n",
    "X_test, _, _, _, _ = engineer_features(\n",
    "    df=test,\n",
    "    is_train=False,\n",
    "    target_series=None,\n",
    "    vectorizer=vectorizer,\n",
    "    mean_listen_time=mean_listen_time,\n",
    "    mean_genre=mean_genre,\n",
    "    scaler=scaler\n",
    ")\n",
    "\n",
    "y = train['Listening_Time_minutes']\n",
    "\n",
    "# Define models\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=101,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.01018372008142483,\n",
    "    tree_method='hist',\n",
    "    eval_metric='mae',\n",
    "    reg_lambda=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "lgb = LGBMRegressor(\n",
    "    n_estimators=320,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.22042348987717186,\n",
    "    force_row_wise=True,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=20,\n",
    "    reg_lambda=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Cross-validation setup\n",
    "print(\"Starting cross-validation...\")\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "stacking_preds = np.zeros(X.shape[0])\n",
    "test_preds = np.zeros(X_test.shape[0])\n",
    "rmses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y.iloc[train_idx]\n",
    "    y_val = y.iloc[val_idx]\n",
    "\n",
    "    # Train individual models\n",
    "    xgb.fit(X_train, y_train)\n",
    "    lgb.fit(X_train, y_train)\n",
    "\n",
    "    # Stacking\n",
    "    stacking = StackingRegressor(\n",
    "        estimators=[\n",
    "            ('xgb', XGBRegressor(\n",
    "                n_estimators=101,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.01018372008142483,\n",
    "                tree_method='hist',\n",
    "                eval_metric='mae',\n",
    "                reg_lambda=0.1,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                objective='reg:squarederror'\n",
    "            )),\n",
    "            ('lgb', LGBMRegressor(\n",
    "                n_estimators=320,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.22042348987717186,\n",
    "                force_row_wise=True,\n",
    "                num_leaves=31,\n",
    "                min_child_samples=20,\n",
    "                reg_lambda=0.1,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ],\n",
    "        final_estimator=XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            tree_method='hist',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        cv=5\n",
    "    )\n",
    "    stacking.fit(X_train, y_train)\n",
    "\n",
    "    # Validate\n",
    "    val_preds = stacking.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "    print(f'Fold {fold + 1} RMSE: {rmse}')\n",
    "    rmses.append(rmse)\n",
    "\n",
    "    # Store predictions\n",
    "    stacking_preds[val_idx] = val_preds\n",
    "    test_preds += stacking.predict(X_test) / kf.n_splits\n",
    "\n",
    "# Blend with a single LightGBM model\n",
    "print(\"Training final LightGBM model...\")\n",
    "lgb_final = LGBMRegressor(\n",
    "    n_estimators=320,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.22042348987717186,\n",
    "    force_row_wise=True,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=20,\n",
    "    reg_lambda=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgb_final.fit(X, y)\n",
    "lgb_test_preds = lgb_final.predict(X_test)\n",
    "final_test_preds = 0.7 * test_preds + 0.3 * lgb_test_preds\n",
    "\n",
    "# Print CV RMSE\n",
    "print(f'Mean CV RMSE: {np.mean(rmses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4080efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file...\n",
      "Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Create submission\n",
    "print(\"Creating submission file...\")\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['Listening_Time_minutes'] = final_test_preds\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created: submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf57de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
